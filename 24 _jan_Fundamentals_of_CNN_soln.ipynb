{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a6d0b56",
   "metadata": {},
   "source": [
    "# 1. Difference between Object Detection and Object Classification.\n",
    "- a. Explain the difference between object detection and object classification in the context of computer vision tasks. Provide examples to illustrate each concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df0175",
   "metadata": {},
   "source": [
    "Object Detection and Object Classification are two related but distinct tasks in the field of computer vision. Let's explore the differences between them:\n",
    "\n",
    "### Object Detection:\n",
    "\n",
    "1. **Definition:**\n",
    "   - **Object Detection** involves identifying and localizing multiple objects within an image or a video frame. It not only categorizes objects but also provides information about their locations.\n",
    "\n",
    "2. **Output:**\n",
    "   - The output of object detection includes bounding boxes around the detected objects along with their corresponding class labels.\n",
    "\n",
    "3. **Examples:**\n",
    "   - Detecting and locating multiple objects in an image, such as identifying and drawing bounding boxes around cars, pedestrians, and bicycles in a street scene.\n",
    "   - Tracking objects in a video stream, where the algorithm continuously updates the positions of objects over time.\n",
    "\n",
    "4. **Challenges:**\n",
    "   - Object detection is a more complex task than classification because it requires handling multiple instances of objects and dealing with overlapping or occluded objects.\n",
    "\n",
    "### Object Classification:\n",
    "\n",
    "1. **Definition:**\n",
    "   - **Object Classification** involves assigning a single label or category to an entire image or an object within an image.\n",
    "\n",
    "2. **Output:**\n",
    "   - The output of object classification is a single label indicating the category or class of the entire image or object.\n",
    "\n",
    "3. **Examples:**\n",
    "   - Classifying an image of a dog as a \"dog\" or a cat as a \"cat.\"\n",
    "   - Assigning a label to a specific region of interest in an image, such as recognizing a handwritten digit in a digit recognition task.\n",
    "\n",
    "4. **Challenges:**\n",
    "   - Object classification is generally considered a simpler task compared to detection because it involves analyzing the content of an entire image or a predefined region without considering the spatial distribution of multiple objects.\n",
    "\n",
    "### Illustrative Example:\n",
    "\n",
    "- **Object Detection Example:**\n",
    "  - Imagine a self-driving car navigating a city street. Object detection helps the car identify and locate various objects simultaneously, such as pedestrians, other vehicles, traffic signs, and obstacles.\n",
    "\n",
    "- **Object Classification Example:**\n",
    "  - In the same scenario, object classification would involve recognizing and categorizing individual objects within the scene. For instance, recognizing a specific car model, identifying the type of traffic sign, or classifying pedestrians as adults or children.\n",
    "\n",
    "Object detection focuses on locating and classifying multiple objects within an image, while object classification involves assigning a single label to an entire image or a specific region of interest. Both tasks are crucial in computer vision applications, and they are often used in combination for comprehensive scene understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c207d7d2",
   "metadata": {},
   "source": [
    "# 2. Scenarios where Object Detection is used:\n",
    "- a. Describe at least three scenarios or real-world applications where object detection techniques are commonly used. Explain the significance of object detection in these scenarios and how it benefits the respective applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc405d5",
   "metadata": {},
   "source": [
    "Object detection techniques play a crucial role in various real-world scenarios, providing solutions to a wide range of applications. Here are three scenarios where object detection is commonly used:\n",
    "\n",
    "1. **Autonomous Vehicles and Traffic Management:**\n",
    "   - **Significance:**\n",
    "     - Object detection is fundamental for the safe operation of autonomous vehicles. It enables the vehicle to perceive and understand its surroundings, identifying and localizing various objects such as pedestrians, other vehicles, traffic signs, and obstacles.\n",
    "   - **Benefits:**\n",
    "     - Improved Safety: Object detection helps prevent collisions by detecting and tracking objects in the vehicle's path.\n",
    "     - Efficient Navigation: Autonomous vehicles use object detection to make informed decisions about changing lanes, navigating intersections, and responding to dynamic traffic conditions.\n",
    "     - Traffic Management: Object detection is also applied in smart traffic management systems to monitor and optimize traffic flow, detect accidents, and enforce traffic regulations.\n",
    "\n",
    "2. **Surveillance and Security:**\n",
    "   - **Significance:**\n",
    "     - Object detection is a key technology in video surveillance systems, enhancing security by identifying and tracking objects or individuals of interest in real-time.\n",
    "   - **Benefits:**\n",
    "     - Threat Detection: Object detection is used to identify and track suspicious activities, intruders, or unattended objects in public spaces, airports, and critical infrastructure.\n",
    "     - Access Control: Object detection helps control access to secure areas by recognizing and verifying individuals based on their appearance or behavior.\n",
    "     - Crowd Monitoring: In crowded areas, object detection aids in monitoring crowd movement and identifying anomalies, ensuring public safety.\n",
    "\n",
    "3. **Retail and Inventory Management:**\n",
    "   - **Significance:**\n",
    "     - Object detection is employed in retail environments for inventory management, customer experience enhancement, and loss prevention.\n",
    "   - **Benefits:**\n",
    "     - Shelf Monitoring: Retailers use object detection to monitor the availability of products on store shelves, ensuring items are in the right place and restocking when necessary.\n",
    "     - Customer Analytics: Object detection helps analyze customer behavior, such as tracking the movement of shoppers and identifying popular product zones within a store.\n",
    "     - Theft Prevention: By detecting suspicious activities or unusual behavior, object detection contributes to loss prevention by identifying potential theft or shoplifting incidents.\n",
    "\n",
    "These scenarios demonstrate the versatility and importance of object detection in diverse applications. Object detection technologies not only enhance efficiency and safety but also contribute to creating smarter, more responsive systems across various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a21cad",
   "metadata": {},
   "source": [
    "# 3. Image Data as Structured Data:\n",
    "- a. Discuss whether image data can be considered a structured form of data. Provide reasoning and examples to support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dad671f",
   "metadata": {},
   "source": [
    "Image data is typically considered unstructured data because it lacks a clear, predefined organization or structure. Unlike structured data, which is organized into rows and columns with a well-defined schema (e.g., tables in a relational database), image data consists of pixel values arranged in a grid. Each pixel represents a color or intensity at a specific location in the image.\n",
    "\n",
    "Here are some reasons why image data is considered unstructured:\n",
    "\n",
    "1. **Pixel Arrangement:**\n",
    "   - Image data is composed of pixels arranged in a 2D or 3D grid, depending on whether it's a grayscale or color image. The relationship between neighboring pixels encodes the spatial information in the image, but there is no inherent structure akin to rows and columns found in structured data.\n",
    "\n",
    "2. **Complexity and Heterogeneity:**\n",
    "   - Image data is highly complex and can be heterogeneous. Images can contain varying shapes, sizes, and colors, making it challenging to represent them in a tabular structure with consistent attributes.\n",
    "\n",
    "3. **Lack of Explicit Semantics:**\n",
    "   - In structured data, each column often represents a specific attribute, and rows correspond to individual records. Image data, on the other hand, lacks explicit semantics for each pixel. While certain patterns or structures may emerge, there is no inherent meaning associated with individual pixel values.\n",
    "\n",
    "4. **Variable Size:**\n",
    "   - Images can have different dimensions, making it impractical to represent them as fixed-size rows or columns. For instance, a dataset of images might include pictures with varying resolutions, requiring a flexible data representation.\n",
    "\n",
    "However, despite being considered unstructured, advancements in computer vision and deep learning have allowed researchers and practitioners to extract meaningful information from image data. Techniques such as convolutional neural networks (CNNs) can automatically learn hierarchical features and patterns from raw pixel data, enabling tasks like image classification, object detection, and segmentation.\n",
    "\n",
    "While image data itself is unstructured, the features extracted through deep learning models can be used in conjunction with structured data or integrated into structured databases for more comprehensive analysis. For example, a database might include structured information about images (e.g., metadata, labels) alongside the raw pixel data, providing a hybrid approach to managing image data within a structured framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef540f",
   "metadata": {},
   "source": [
    "# 4. Explaining Information in an Image for CNN:\n",
    "- a. Explain how Convolutional Neural Networks (CNN) can extract and understand information from an image. Discuss the key components and processes involved in analyzing image data using CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398f8c81",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks (CNNs) are a class of deep neural networks designed to process and analyze visual data, making them particularly effective for tasks such as image recognition, object detection, and segmentation. CNNs excel at automatically learning hierarchical features from raw pixel data, enabling them to understand complex patterns within images. Here's an explanation of how CNNs extract and understand information from an image:\n",
    "\n",
    "### Key Components and Processes of CNNs:\n",
    "\n",
    "1. **Convolutional Layers:**\n",
    "   - **Convolution Operation:** The core operation in CNNs is convolution, where filters (also known as kernels) slide across the input image, extracting local patterns or features. Convolutional layers apply multiple filters to capture different aspects of the image.\n",
    "   - **Feature Maps:** The result of the convolution operation is a set of feature maps, each highlighting specific features present in the input image.\n",
    "\n",
    "2. **Activation Function:**\n",
    "   - After convolution, an activation function (commonly ReLU - Rectified Linear Unit) is applied element-wise to the feature maps. This introduces non-linearity to the network, allowing it to learn more complex patterns and relationships in the data.\n",
    "\n",
    "3. **Pooling Layers:**\n",
    "   - Pooling layers (e.g., MaxPooling or AveragePooling) reduce the spatial dimensions of the feature maps by downsampling. This helps in retaining important information while reducing the computational load and increasing the network's translation invariance.\n",
    "\n",
    "4. **Flattening:**\n",
    "   - After several convolutional and pooling layers, the data is flattened into a one-dimensional vector. This vector serves as the input to fully connected layers.\n",
    "\n",
    "5. **Fully Connected Layers:**\n",
    "   - In the fully connected layers, the network learns global patterns and relationships from the extracted features. These layers connect every neuron to every neuron in the preceding and succeeding layers.\n",
    "\n",
    "6. **Output Layer:**\n",
    "   - The final layer produces the network's output. For image classification, this often involves a softmax activation function, which assigns probabilities to different classes.\n",
    "\n",
    "### Feature Hierarchy:\n",
    "\n",
    "- **Local Features:** Early layers in the network capture local features such as edges, textures, and simple patterns. Convolutional layers closer to the input learn these local features.\n",
    "\n",
    "- **Mid-Level Features:** As the data progresses through the network, higher-level features and representations emerge. These might include more complex shapes, parts of objects, or textures.\n",
    "\n",
    "- **Global Features:** Fully connected layers at the end of the network analyze the combination of mid-level features, forming a global representation of the input image. This global representation is used for making high-level decisions, such as image classification.\n",
    "\n",
    "### Training Process:\n",
    "\n",
    "- CNNs are trained using labeled datasets through a process called backpropagation. The model adjusts its weights and biases to minimize the difference between predicted and actual labels. This process enables the network to learn to recognize patterns and features relevant to the task at hand.\n",
    "\n",
    "### Transfer Learning:\n",
    "\n",
    "- CNNs can benefit from transfer learning, where a pre-trained model on a large dataset is fine-tuned for a specific task with a smaller dataset. This is especially useful when training data is limited.\n",
    "\n",
    "CNNs extract information from images by learning hierarchical features through convolutional operations, activation functions, pooling layers, and fully connected layers. This hierarchical feature extraction allows CNNs to automatically understand and recognize complex patterns within images, making them powerful tools for various computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c481349b",
   "metadata": {},
   "source": [
    "# 5.Flattening Images for ANN:\n",
    "- a. Discuss why it is not recommended to flatten images directly and input them into an Artificial Neural Network (ANN) for image classification. Highlight the limitations and challenges associated with this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b319cb",
   "metadata": {},
   "source": [
    "Flattening images and directly inputting them into an Artificial Neural Network (ANN) for image classification is generally not recommended due to several limitations and challenges. Here are some reasons why this approach is not ideal:\n",
    "\n",
    "1. **Loss of Spatial Information:**\n",
    "   - Flattening the image removes the spatial structure of the data. In an image, neighboring pixels often contain valuable spatial information and relationships that are important for understanding the content of the image. Flattening discards this spatial arrangement, which can be crucial for tasks like object detection or segmentation.\n",
    "\n",
    "2. **Ignoring Local Patterns and Features:**\n",
    "   - Images typically contain local patterns and features at different scales. Flattening discards the 2D or 3D structure of the image, making it difficult for the neural network to capture and leverage local features effectively. Convolutional layers in Convolutional Neural Networks (CNNs) are designed to address this limitation by extracting hierarchical features from local receptive fields.\n",
    "\n",
    "3. **Increased Number of Parameters:**\n",
    "   - Flattening results in a large 1D vector for each image, leading to a high-dimensional input space. This, in turn, increases the number of parameters in the fully connected layers of the ANN. Training a neural network with a large number of parameters requires more data, and it is more prone to overfitting, especially when the dataset is limited.\n",
    "\n",
    "4. **Computational Inefficiency:**\n",
    "   - Flattening images results in a considerable increase in the number of input neurons for fully connected layers. This leads to increased computational complexity during both training and inference, making the network computationally inefficient, especially for large images.\n",
    "\n",
    "5. **Lack of Translation Invariance:**\n",
    "   - Flattening ignores the concept of translation invariance, which is crucial for recognizing objects in different positions within an image. Convolutional layers in CNNs capture translation-invariant features by using shared weights across different spatial locations.\n",
    "\n",
    "6. **Limited Reusability and Transfer Learning:**\n",
    "   - Pre-trained convolutional layers in CNNs can be used for transfer learning, allowing the model to leverage knowledge learned from large datasets. Flattening images directly eliminates the possibility of using pre-trained convolutional layers, limiting the reusability and transferability of learned features.\n",
    "\n",
    "7. **Not Robust to Image Variations:**\n",
    "   - Flattening does not handle variations in image size or aspect ratio well. CNNs, on the other hand, can adapt to different input sizes and learn hierarchical features that are robust to variations.\n",
    "\n",
    "flattening images and using them as inputs to an ANN for image classification neglects the inherent spatial structure and patterns in images. Convolutional Neural Networks (CNNs) are specifically designed to address these challenges, allowing them to automatically learn hierarchical features and relationships from raw pixel data, making them more suitable for image-related tasks. CNNs provide better performance, efficiency, and generalization capabilities compared to ANNs when applied to image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c1f93c",
   "metadata": {},
   "source": [
    "# 6. Applying CNN to the MNIST Dataset:\n",
    "- a. Explain why it is not necessary to apply CNN to the MNIST dataset for image classification. Discuss the characteristics of the MNIST dataset and how it aligns with the requirements of CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bbfb33",
   "metadata": {},
   "source": [
    "The MNIST dataset consists of grayscale images of handwritten digits (0 to 9), each of size 28x28 pixels. Given the simplicity and low resolution of the MNIST images, applying Convolutional Neural Networks (CNNs) is not always necessary or may not provide significant advantages. Here are some reasons why CNNs are not essential for the MNIST dataset:\n",
    "\n",
    "1. **Low Spatial Resolution:**\n",
    "   - MNIST images are small, with a spatial resolution of only 28x28 pixels. CNNs are particularly effective when dealing with high-resolution images where spatial hierarchies and local patterns play a crucial role. In the case of MNIST, the limited spatial information can be adequately captured by fully connected layers in a traditional Artificial Neural Network (ANN).\n",
    "\n",
    "2. **Limited Complexity and Variability:**\n",
    "   - MNIST digits are relatively simple and lack the complex structures and variations present in natural images. Convolutional layers in CNNs are designed to capture hierarchical features in more complex and varied images. For MNIST, simpler architectures like fully connected networks can often learn the necessary representations effectively.\n",
    "\n",
    "3. **Translation Invariance Not Critical:**\n",
    "   - CNNs are well-suited for tasks requiring translation invariance, where the location of features within an image is important. MNIST digits are centered in the images, and their classification is not highly sensitive to small translations. As a result, the translation-invariant properties of CNNs may not provide significant benefits in this context.\n",
    "\n",
    "4. **Parameter Efficiency:**\n",
    "   - The small size of MNIST images allows fully connected networks to have a relatively small number of parameters, making them computationally efficient. CNNs, especially with large convolutional layers, may introduce unnecessary complexity for such a simple dataset.\n",
    "\n",
    "5. **Overhead of Convolutional Operations:**\n",
    "   - The convolutional operations in CNNs come with additional computational overhead. For MNIST, where the information content is concentrated in a small area, convolutional operations might not be as crucial as in more intricate image datasets.\n",
    "\n",
    "However, it's worth noting that while CNNs might not be strictly necessary for MNIST, they can still be applied and may provide slightly improved performance or faster convergence. Some researchers have demonstrated that simple CNN architectures can achieve high accuracy on MNIST, leveraging the ability of CNNs to capture hierarchical features.\n",
    "\n",
    "while CNNs are powerful tools for image-related tasks, they may not be essential for datasets with simple and low-resolution images, such as MNIST. Fully connected networks can often achieve satisfactory results on MNIST, and the additional complexity introduced by CNNs may not be justified for this specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5609eebd",
   "metadata": {},
   "source": [
    "# 7. Extracting Features at Local Space:\n",
    "- a. Justify why it is important to extract features from an image at the local level rather than considering the entire image as a whole. Discuss the advantages and insights gained by performing local feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25758772",
   "metadata": {},
   "source": [
    "Extracting features at the local level rather than considering the entire image as a whole is crucial in computer vision for several reasons. Local feature extraction provides a more detailed and context-aware representation of an image, offering numerous advantages and insights. Here are some justifications for the importance of local feature extraction:\n",
    "\n",
    "1. **Robustness to Spatial Variations:**\n",
    "   - Local feature extraction allows the model to be robust to spatial variations within an image. Objects or patterns of interest may appear at different positions, scales, or orientations. By focusing on local regions, the model becomes invariant to translations and rotations, making it more adaptable to different spatial arrangements.\n",
    "\n",
    "2. **Translation Invariance:**\n",
    "   - Local feature extraction enables the model to achieve translation invariance, meaning it can recognize objects regardless of their specific position in the image. Convolutional Neural Networks (CNNs), for example, use local convolutional operations to capture local patterns across different spatial locations.\n",
    "\n",
    "3. **Handling Occlusions:**\n",
    "   - In the presence of occlusions or overlapping objects, local feature extraction allows the model to focus on the discernible features within a specific region. This is particularly important for tasks like object detection, where objects may partially occlude each other in an image.\n",
    "\n",
    "4. **Hierarchical Feature Learning:**\n",
    "   - Local features serve as the building blocks for hierarchical feature learning. Instead of analyzing the entire image at once, the model learns to recognize local patterns first and then combines them to form higher-level representations. This hierarchical approach captures complex structures and relationships in the data.\n",
    "\n",
    "5. **Improved Generalization:**\n",
    "   - Local features help improve the generalization of a model across diverse datasets. Models that rely on local information are more likely to generalize well to new images with different spatial arrangements, scales, or viewpoints.\n",
    "\n",
    "6. **Efficient Computational Processing:**\n",
    "   - Analyzing the entire image at once can be computationally expensive. Local feature extraction, especially when implemented with techniques like convolutional operations, allows for efficient processing by focusing computational resources on smaller regions of interest.\n",
    "\n",
    "7. **Sparse Data Representation:**\n",
    "   - Local features provide a more sparse representation of the image, emphasizing only the relevant details. This can reduce the risk of overfitting and improve the model's ability to generalize to unseen data.\n",
    "\n",
    "8. **Semantic Interpretation:**\n",
    "   - Local features contribute to a more semantically meaningful interpretation of an image. Instead of treating the entire image as a single entity, the model learns to recognize specific objects, textures, or patterns in different regions, contributing to a richer understanding of the image content.\n",
    "\n",
    "local feature extraction is essential for addressing challenges related to spatial variations, translation invariance, occlusions, and hierarchical learning. By focusing on local regions, computer vision models can achieve better robustness, generalization, and efficiency in analyzing diverse and complex visual data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009087da",
   "metadata": {},
   "source": [
    "# 8. Importance of Convolution and Max Pooling:\n",
    "- a. Elaborate on the importance of convolution and max pooling operations in a Convolutional Neural Network (CNN). Explain how these operations contribute to feature extraction and spatial down-sampling in CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a4610",
   "metadata": {},
   "source": [
    "Convolution and max pooling operations are fundamental building blocks in Convolutional Neural Networks (CNNs) and play a crucial role in feature extraction and spatial down-sampling. Here's an elaboration on the importance of these operations:\n",
    "\n",
    "### 1. Convolution Operation:\n",
    "\n",
    "#### Importance for Feature Extraction:\n",
    "\n",
    "- **Local Feature Learning:**\n",
    "  - Convolution allows the network to learn local features by applying filters (kernels) to small receptive fields of the input image. These filters capture patterns, edges, textures, and other important features within the local context.\n",
    "\n",
    "- **Parameter Sharing:**\n",
    "  - Convolutional operations involve parameter sharing, where the same filter is applied across different spatial locations. This helps in learning translation-invariant features and reduces the number of parameters compared to fully connected networks, making CNNs computationally efficient.\n",
    "\n",
    "- **Hierarchical Feature Learning:**\n",
    "  - Stacking multiple convolutional layers allows the network to learn hierarchical features. Lower layers capture basic patterns, and higher layers combine these features to form more complex representations. This hierarchical learning is crucial for understanding the visual hierarchy in complex images.\n",
    "\n",
    "- **Spatial Relationships:**\n",
    "  - Convolution takes into account the spatial relationships between pixels, preserving the spatial structure of the input. This is essential for capturing spatial hierarchies and understanding how different features relate to each other.\n",
    "\n",
    "### 2. Max Pooling Operation:\n",
    "\n",
    "#### Importance for Spatial Down-sampling:\n",
    "\n",
    "- **Reducing Spatial Dimensions:**\n",
    "  - Max pooling is a down-sampling operation that reduces the spatial dimensions of the feature maps. It involves taking the maximum value from a group of neighboring pixels. This reduction helps in decreasing the computational load and focusing on the most salient features.\n",
    "\n",
    "- **Translation Invariance:**\n",
    "  - Max pooling contributes to translation invariance by selecting the most prominent features within local regions. Even if an object moves slightly within the receptive field, the max-pooled output remains relatively stable, contributing to robustness against translations.\n",
    "\n",
    "- **Feature Invariance:**\n",
    "  - Max pooling provides a certain degree of feature invariance, making the model less sensitive to small changes in the input. This is particularly beneficial for tasks where the exact position of features is less important, such as object recognition.\n",
    "\n",
    "- **Handling Variations in Scale:**\n",
    "  - Max pooling helps handle variations in scale by retaining the most important information and discarding less relevant details. It allows the model to focus on capturing the core features while discarding fine-grained variations.\n",
    "\n",
    "- **Reducing Overfitting:**\n",
    "  - Spatial down-sampling through max pooling helps reduce overfitting by creating a more abstract representation of the input. It retains the essential information while discarding unnecessary details, promoting better generalization.\n",
    "\n",
    "Convolution and max pooling operations are integral to the success of CNNs. Convolution enables local feature learning and hierarchical representation, while max pooling contributes to spatial down-sampling, translation invariance, and feature abstraction. These operations collectively empower CNNs to efficiently learn and recognize complex patterns in visual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03e4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
